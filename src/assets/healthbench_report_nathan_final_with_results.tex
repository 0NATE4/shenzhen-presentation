\documentclass[a4paper,12pt]{article}

% --- Packages ---
\usepackage{tabularx}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{parskip}

% --- Code Block Styling ---
\definecolor{codegray}{rgb}{0.95,0.95,0.95}
\definecolor{codeblue}{rgb}{0.1,0.1,0.6}
\lstset{
    backgroundcolor=\color{codegray},
    basicstyle=\ttfamily\small,
    breaklines=true,
    keywordstyle=\color{codeblue}\bfseries,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

% --- Title Information (EDIT THESE) ---
\title{\textbf{HealthBench Evaluation Optimisation for Qwen3-4B-Thinking-2507}\\ \large OpenCompass baseline analysis and an async evaluator speed-up}
\author{\textbf{Nathan Chung} \\ \textit{Internship report}}
\date{16 December 2025}

\begin{document}

\maketitle
\thispagestyle{empty}

\begin{abstract}
\noindent
Evaluating large language models for healthcare requires realistic test data and reliable grading.
HealthBench is a rubric-based benchmark that scores model responses to health-related conversations using a separate judge model, then reports accuracy and uncertainty estimates.
OpenCompass provides an end-to-end framework for running HealthBench, including dataset loading, model inference, and judge-based evaluation.
In this project, the standard OpenCompass HealthBench pipeline for \texttt{Qwen3-4B-Thinking-2507} was too slow for practical iteration and, in practice, could stall due to unbounded judge retry behaviour when invalid JSON was returned.
To address this, I built a standalone async HealthBench evaluator that preserves OpenCompass prompt templates, parsing rules, and scoring logic while increasing judge throughput via \texttt{asyncio}, bounded retries, and resumable JSONL logging.
On a representative subset (\texttt{dev\_100}), the async evaluator produced the same base-subset accuracy as OpenCompass and completed inference plus evaluation in under 20 minutes at \texttt{concurrency=200}.
On the full dataset, the optimised pipeline reduced end-to-end runtime from an estimated four days to about 13 hours.
On the full dataset, the final run achieved 0.5094 accuracy on the base subset (n=5000), 0.2298 on the hard subset (n=1000), and 0.7274 on the consensus subset (n=3671), with bootstrap uncertainty.
\end{abstract}

\newpage
\setcounter{page}{1}
\tableofcontents
\newpage

% =========================
\section{Goals and context}
% =========================

\subsection{Key internship tasks}
This work was organised around three goals.

\begin{itemize}
    \item Learn common LLM evaluation approaches and the OpenCompass evaluation framework.
    \item Understand the HealthBench medical benchmark, including rubric design and judge-based scoring.
    \item Evaluate \texttt{Qwen3-4B-Thinking-2507} on HealthBench using OpenCompass, while making the pipeline fast enough for practical iteration.
\end{itemize}

\subsection{Why optimisation was needed}
Large language model evaluation requires both realistic test data and reliable grading.
HealthBench evaluates models on health-related conversations using multi-item rubrics.
A separate judge model scores each assistant response against these rubrics and aggregates accuracy statistics with bootstrap uncertainty estimates.
OpenCompass is the framework used in this project to run HealthBench.
It manages dataset loading, model inference, and evaluation against the judge.
For the \texttt{Qwen3-4B-Thinking-2507} model, the standard OpenCompass HealthBench pipeline can take about four days for a full run.
This makes it impractical to test new judge models or tune inference settings in a tight loop.

% =========================
\section{Background}
% =========================

\subsection{HealthBench overview}
HealthBench consists of multi-turn conversations and conversation-specific rubrics authored by physician experts.
Each example contains a set of rubric items, each worth a number of points.
A judge model is prompted with the conversation, the rubric item, and the candidate completion, and returns a JSON object including whether criteria were met.
Scores are aggregated into subset-level accuracy (base, hard, consensus), with uncertainty estimated via bootstrapping.

\subsection{OpenCompass overview}
OpenCompass runs evaluation as a pipeline with two main stages.

\begin{itemize}
    \item Inference: run the student model and write predictions to disk.
    \item Evaluation: call the judge model to grade predictions and compute metrics.
\end{itemize}

OpenCompass supports a range of model backends and partitioning strategies, and HealthBench evaluation is configured via Python config files that define the student model, judge model, datasets, and evaluator parameters.

% =========================
\section{Baseline problem analysis}
% =========================

\subsection{Observed behaviour}
A baseline OpenCompass HealthBench run was observed to take multiple days and, in at least one run, stalled without writing new evaluation outputs for an extended period.
The stall coincided with repeated judge failures and a retry loop.

\subsection{Primary root cause: unbounded retries on invalid JSON}
The main reliability issue was unbounded retries on judge responses that could not be parsed as valid JSON.
In a representative baseline run, there were 106 JSON decode failures out of 539 judge calls, which is about 20\%.
With a non-zero probability of persistent failure, infinite retries can deadlock progress because some fraction of items never succeed, so workers remain stuck rather than advancing through the dataset.

\subsection{Additional stall mode: exponential backoff on non-recoverable errors}
In addition to invalid JSON outputs, OpenCompass wraps judge calls in a sampler that retries indefinitely with exponential backoff and a large per-request timeout (default 600 seconds).
If a failure is non-recoverable, such as a request that exceeds context length or token limits, retrying cannot succeed and the backoff can grow without bound, effectively hanging evaluation.
This risk increases when rare examples exceed the configured maximum output length (for example 16,384 tokens), or when long conversations cause the judge prompt to exceed model limits.
The async evaluator avoids this by using bounded retries, explicit timeouts, and recording failures so the run can always make forward progress.
Relevant OpenCompass excerpts are included in Appendix~
ef{app:oc-code}.

\subsection{Secondary contributing factors}
Even when judge responses were valid, the baseline evaluation path had significant orchestration overhead.
It used a process pool plus per-process thread pools and made a high volume of small HTTP calls.
This structure made it difficult to push concurrency high enough for a fast evaluation loop.

% =========================
\section{Solution overview}
% =========================

\subsection{Approach}
I kept OpenCompass for dataset loading and student-model inference, then replaced the evaluation stage with a standalone async evaluator.
The design goals were:

\begin{itemize}
    \item Preserve functional equivalence with OpenCompass scoring.
    \item Increase judge throughput using asynchronous HTTP calls.
    \item Guarantee forward progress via bounded retries and per-call timeouts.
    \item Support resuming long runs without losing already-scored items.
\end{itemize}

\subsection{End to end workflow}
\begin{itemize}
    \item Run OpenCompass in inference-only mode to generate prediction JSON files.
    \item Load the matching HealthBench dataset JSONL and align predictions to examples.
    \item For each example and rubric item, call the judge model concurrently.
    \item Parse judge outputs, compute per-example scores, then aggregate and estimate uncertainty via bootstrapping.
    \item Write results JSON and summaries, and maintain a streaming JSONL judge log for observability and resume.
\end{itemize}

% =========================
\section{Implementation details}
% =========================

\subsection{Core components}
The optimised evaluation pipeline consists of four main components.

\begin{itemize}
    \item \texttt{healthbench\_infer\_config.py}: OpenCompass wrapper config for inference-only runs.
    \item \texttt{async\_healthbench\_eval.py}: async evaluator CLI driving judge calls at high throughput.
    \item \texttt{healthbench\_scoring.py}: scoring logic extracted from OpenCompass for equivalence.
    \item \texttt{run\_healthbench\_full.sh}: wrapper script that orchestrates inference and evaluation end to end.
\end{itemize}

\subsection{Shard reindexing for OpenCompass predictions}
OpenCompass may shard predictions into multiple files such as \texttt{healthbench\_0.json}, \texttt{healthbench\_1.json}, and so on.
Each shard uses keys starting from \texttt{"0"}, so naively merging them causes collisions and silent overwrites.
The evaluator uses cumulative offset reindexing to preserve a position-based join with the dataset.

\begin{lstlisting}[language=Python,caption={Shard reindexing via cumulative offset}]
offset = 0
for json_file in sorted_by_shard_number(matched_files):
    data = json.load(open(json_file))
    reindexed = {str(int(k) + offset): v for k, v in data.items()}
    merged.update(reindexed)
    offset += len(data)
\end{lstlisting}

\subsection{Pattern-based subset file matching}
To support both single prediction files and sharded prediction directories, the evaluator uses regex patterns for subset discovery.

\begin{lstlisting}[language=Python,caption={Prediction file patterns for base, hard, consensus}]
PREDICTION_PATTERNS = {
    "base": r"^healthbench(_\d+)?\.json$",
    "hard": r"^healthbench_hard(_\d+)?\.json$",
    "consensus": r"^healthbench_consensus(_\d+)?\.json$",
}
\end{lstlisting}

\subsection{Async concurrency model}
Judge calls are made using an async OpenAI-compatible client, with a semaphore to cap the number of in-flight requests.

\begin{lstlisting}[language=Python,caption={Async semaphore for bounded concurrency}]
sem = asyncio.Semaphore(concurrency)  # default: 200

async with sem:
    response_text, error = await judge_call(...)
\end{lstlisting}

\subsection{Resume support via streaming JSONL}
All judge outputs are written to a JSONL log so a run can be resumed without re-scoring completed (prompt\_id, rubric\_index) pairs.

\begin{lstlisting}[caption={Example JSONL record (one per judge call)}]
{"prompt_id": "...", "rubric_index": 3, "rubric_points": 2.0,
 "criteria_met": true, "explanation": "...", "raw_response": "..."}
\end{lstlisting}

\subsection{Error handling and retry policy}
To avoid deadlocks, judge calls use bounded retries and per-call timeouts.
The default policy is \texttt{max\_retries=3} with exponential backoff of 1s, 2s, 4s on rate limits, transient server errors, and timeouts.
When retries are exhausted, the evaluator records a structured failure and continues, which guarantees forward progress.

\subsection{Postprocessing to remove reasoning}
HealthBench grading should target the final answer rather than chain-of-thought.
For thinking models that emit \texttt{<think>...</think>} blocks, the pipeline applies \texttt{extract\_non\_reasoning\_content} before sending predictions to the judge.
Fixing a bug where reasoning was not stripped improved dev subset accuracy from about 0.06 to about 0.62, aligning with expected HealthBench behaviour.

% =========================
\section{Validation and equivalence}
% =========================

\subsection{Equivalence definition}
Equivalence was defined as follows.
Given identical student predictions and dataset inputs, the async evaluator should generate the same judge prompts and apply the same parsing and scoring logic as OpenCompass.
Any remaining differences in outcomes should be attributable to judge model non-determinism.

\subsection{Matched components}
\begin{itemize}
    \item Judge prompt templates copied byte-for-byte from OpenCompass.
    \item Parsing rules preserved, including expected JSON fields and regex extraction.
    \item Scoring and aggregation logic ported directly, including clipped mean and bootstrap standard deviation.
\end{itemize}

\subsection{dev\_100 validation results}
For \texttt{dev\_100}, the async evaluator produced:

\begin{itemize}
    \item Base accuracy: 0.6217 \(\pm\) 0.0374 (n = 50)
    \item Hard accuracy: 0.2639 \(\pm\) 0.0819 (n = 25)
    \item Consensus accuracy: 0.7200 \(\pm\) 0.0652 (n = 25)
\end{itemize}

Hard and consensus can vary more at small sample sizes, so base subset behaviour is treated as the primary equivalence signal.

% =========================
\section{Results}
% =========================

\subsection{Full dataset timing}
A full dataset run completed with the following pipeline timing.

\renewcommand{\arraystretch}{1.2}
\begin{tabularx}{\textwidth}{X r}
\toprule
\textbf{Stage} & \textbf{Time} \\
\midrule
Inference (student model) & 2h 42m 37s (9,757s) \\
Evaluation (judge scoring) & 10h 24m 23s (37,463s) \\
Total pipeline & 13h 7m 0s (47,220s) \\
\bottomrule
\end{tabularx}

\subsection{Controlled benchmark throughput}
To isolate orchestration overhead, I compared OpenCompass evaluation against a minimal async harness on an identical workload (50 examples, 539 judge calls). HealthBench is graded per rubric item, so each example expands into multiple judge requests (see Appendix~\ref{app:oc-code}).
The async harness and OpenCompass used the same judge endpoint and prompt template, so the difference reflects framework overhead rather than model capacity.

\renewcommand{\arraystretch}{1.2}
\begin{tabularx}{\textwidth}{X r r r}
\toprule
\textbf{Metric} & \textbf{Async harness} & \textbf{OpenCompass} & \textbf{Ratio} \\
\midrule
Time (s) & 222 & 2{,}591 & 11.7x slower \\
Throughput (requests/s) & 2.43 & 0.21 & 0.09x \\
\bottomrule
\end{tabularx}

This shows that OpenCompass can be about twelve times slower than necessary for the same judge workload, which supports the decision to replace the evaluation stage while keeping scoring semantics unchanged.



\subsection{Time savings analysis}
Based on throughput improvement, evaluation time was expected to drop from around four days to around 0.4 days.
The observed evaluation time of 10h 24m closely matched this estimate.

\subsection{Accuracy results}
The final full run used \texttt{concurrency=200} with timestamp \texttt{healthbench\_full\_20251216\_142123}.
Accuracies are reported with bootstrap standard deviation.

\renewcommand{\arraystretch}{1.2}
\begin{tabularx}{\textwidth}{X r r r}
\toprule
\textbf{Subset} & \textbf{Accuracy} & \textbf{Bootstrap std} & \textbf{N} \\
\midrule
healthbench (base) & 0.5094 & 0.0055 & 5000 \\
healthbench\_hard & 0.2298 & 0.0109 & 1000 \\
healthbench\_consensus & 0.7274 & 0.0056 & 3671 \\
\midrule
healthbench\_all (naive average) & 0.4889 & -- & -- \\
\bottomrule
\end{tabularx}

\noindent
The \texttt{healthbench\_all} value is the unweighted mean of the three subset accuracies, matching the report summariser definition.

\subsection{Speed-up summary}
Based on measured timing, the optimised pipeline reduces end-to-end runtime from an estimated four days to about 13 hours, which is about a 7.3x speed-up overall.
A representative subset benchmark also showed an order-of-magnitude judge throughput improvement compared with the baseline evaluation path.

% =========================
\section{Discussion}
% =========================

\subsection{What worked}
\begin{itemize}
    \item Async judge calls increased throughput and improved resource utilisation.
    \item Bounded retries and timeouts prevented deadlocks from persistent judge failures.
    \item JSONL logging improved observability and enabled resuming long runs.
    \item Shard reindexing prevented silent data loss for sharded OpenCompass prediction outputs.
\end{itemize}

\subsection{Limitations and open items}
\begin{itemize}
    \item OpenCompass vanilla full-run evaluation was not completed due to baseline stall risk, so full-run equivalence is supported primarily by \texttt{dev\_100}.
    \item Equivalence is claimed only on \texttt{dev\_100}.
    \item The current join between predictions and dataset entries is position-based.
    \item Parallel subset evaluation and adaptive concurrency tuning are future work.
\end{itemize}

\subsection{Recommendations for practical use}
\begin{itemize}
    \item Use dev subsets for quick sanity checks before a full run.
    \item Monitor judge API latency and error rates when tuning concurrency.
    \item Always enable \texttt{--judge-log} for resume support.
    \item Validate code changes against OpenCompass on a small subset before trusting full runs.
\end{itemize}

% =========================
\section{Conclusion}
% =========================

This project made HealthBench evaluation for \texttt{Qwen3-4B-Thinking-2507} practical by replacing the slow and failure-prone OpenCompass evaluation stage with a standalone async evaluator while preserving scoring logic.
The optimised pipeline achieved a full dataset runtime of 13h 7m, enabling iterative evaluation cycles that were previously impractical.
Full-run accuracy metrics are now included, and the pipeline remains resumable and suitable for repeated evaluations.

\subsection*{Final deliverables}
\begin{itemize}
    \item Optimised async pipeline achieving a 7 to 10x speed-up compared with baseline OpenCompass evaluation.
    \item Technical documentation covering design decisions, implementation, and validation evidence.
    \item Equivalence evidence on \texttt{dev\_100}.
    \item Performance analysis and benchmarking results.
    \item Full dataset HealthBench results for \texttt{healthbench}, \texttt{healthbench\_hard}, and \texttt{healthbench\_consensus}.
\end{itemize}

\subsection*{Future work}
\begin{itemize}
    \item Implement a prompt-id based join once prediction IDs are exposed consistently.
    \item Add optional parallel evaluation of base, hard, and consensus subsets if judge capacity allows.
    \item Add adaptive concurrency that responds to observed judge latency and rate limits.
\end{itemize}

% =========================
\section*{References}
% =========================
\begin{itemize}
    \item OpenCompass repository: \url{https://github.com/open-compass/opencompass}
    \item HealthBench paper (Arora et al., 2025): \url{https://arxiv.org/abs/2505.08775}
    \item Introducing HealthBench (OpenAI): \url{https://openai.com/index/healthbench/}
\end{itemize}

% =========================
\appendix
\section{Appendix: Reproducibility and evidence}
% =========================

\subsection{Environment summary (redacted)}
\begin{itemize}
    \item Hardware: 8 \(\times\) NVIDIA A40
    \item Student model: \texttt{Qwen3-4B-Thinking-2507} served via local vLLM
    \item Judge model: \texttt{qwen3-235b-a22b-thinking-2507} served via a remote OpenAI-compatible HTTP API
    \item Runtime: Python 3.10 in a Conda environment
    \item Key dependencies: OpenCompass, vLLM, OpenAI client library, NumPy, mmengine
\end{itemize}

For security, credentials such as passwords and API keys are intentionally omitted from this report.
They can be shared via internal channels if required.

\subsection{Representative commands (redacted)}
\begin{lstlisting}[language=bash,caption={Judge environment variables (redacted)}]
export OC_JUDGE_MODEL="qwen3-235b-a22b-thinking-2507"
export OC_JUDGE_API_BASE="http://<REDACTED>:30000/v1"
export OC_JUDGE_API_KEY="<REDACTED>"
\end{lstlisting}

\begin{lstlisting}[language=bash,caption={Example wrapper usage}]
./run_healthbench_full.sh qwen3-4b-thinking-2507 full 200
\end{lstlisting}

\subsection{Code repository}
The implementation and documentation are available in a repository.

\begin{itemize}
    \item Repo: \url{<REPO_URL>}
    \item Snapshot: commit \texttt{<GIT_SHA>} (or tag \texttt{<TAG>})
    \item Note: credentials and internal endpoints are not committed and are shared via internal channels.
\end{itemize}

\subsection{Baseline failure evidence (summary)}
\begin{itemize}
    \item Invalid JSON frequency in a representative baseline: 106 JSON decode failures out of 539 judge calls, which triggered repeated retries.
    \item The async evaluator bounds retries and time per call, records failures, and continues, which guarantees forward progress.
\end{itemize}

\subsection{Baseline stall evidence excerpt}
The following excerpts were captured during a stalled baseline run to support root-cause analysis.

\begin{lstlisting}[language=bash,caption={Process snapshot excerpt}]
PID=<REDACTED>
ELAPSED: 3-06:59:56
STATE:   S
%CPU:    0.0
%MEM:    0.1
\end{lstlisting}

\begin{lstlisting}[language=bash,caption={Network state excerpt}]
# Connections associated with the judge API were observed in CLOSE-WAIT
# with no ESTABLISHED connections for the stuck worker processes.
\end{lstlisting}

\begin{lstlisting}[language=bash,caption={py-spy excerpt}]
# Main thread blocked waiting on multiprocessing.pool.next
# At least one worker thread inside the judge call path.
\end{lstlisting}

\section{Appendix: OpenCompass bottleneck code excerpts}
\label{app:oc-code}

\subsection{Judge sampler retry loop}
The OpenCompass HealthBench judge sampler retries indefinitely on generic exceptions, with exponential backoff and a large timeout.

\begin{lstlisting}[language=Python,caption={OpenCompass judge sampler: unbounded retry loop with exponential backoff}]
JUDGE_TIMEOUT = float(os.getenv('HEALTHBENCH_JUDGE_TIMEOUT', '600'))

def __call__(self, message_list: MessageList) -> SamplerResponse:
    ...
    trial = 0
    while True:
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=message_list,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                timeout=JUDGE_TIMEOUT,
            )
            ...
            return SamplerResponse(...)
        except openai.BadRequestError as e:
            return SamplerResponse_toggle_failure(...)
        except Exception as e:
            exception_backoff = 2**trial
            time.sleep(exception_backoff)
            trial += 1
\end{lstlisting}

\subsection{Per-rubric evaluation orchestration}
OpenCompass grades HealthBench per rubric item. This multiplies judge calls because each example can contain many rubric items.

\begin{lstlisting}[language=Python,caption={OpenCompass per-rubric grading: judge called once per rubric item (excerpt)}]
# opencompass/datasets/healthbench/healthbench.py
def _grade_sample_per_rubric(self, convo_with_response, rubric_items):

    def grade_rubric_item(rubric_item):
        messages = [dict(content=grader_prompt, role='user')]
        sampler_response = self.grader_model(messages)
        grading_response = sampler_response.response_text
        grading_response_dict = parse_json_to_dict(grading_response)
        return grading_response_dict

    return [grade_rubric_item(item) for item in rubric_items]
\end{lstlisting}


\end{document}